{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### TMA4205 Numerical Linear Algebra\n",
    "# Project Part 2\n",
    "## Introduction\n",
    "\n",
    "In this part we want to find a low rank continuous approximation to a continuous matrix valued function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports and useful functions\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "\n",
    "# line cyclers adapted to colourblind people\n",
    "from cycler import cycler\n",
    "\n",
    "line_cycler = (cycler(color=[\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#F0E442\"]) +\n",
    "               cycler(linestyle=[\"-\", \"--\", \"-.\", \":\", \"-\", \"--\", \"-.\"]))\n",
    "plt.rc(\"axes\", prop_cycle=line_cycler)\n",
    "plt.rc('axes', axisbelow=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"First define some useful functions for plotting and testing\"\"\"\n",
    "from linalg.plotting import *\n",
    "from linalg.helpers import get_function_timings, truncated_svd, get_equidistant_indexes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1:\n",
    "We implement Lanczos bidiagonalization method with and without re-orthogonalization."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from linalg.bidiagonalization import lanczos_bidiag, lanczos_bidiag_reorth, make_bidiagonal, get_bidiagonal_approx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make 3 randomly generated matrices $A_n \\in \\mathbb{R}^{n\\times n} $, $n \\in [32,64,128]$.\n",
    "Then we show their eigenvalues."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make matrices\n",
    "n_list = [32, 64, 128]\n",
    "A_list = [np.random.rand(n, n) * 2 - 1 for n in n_list]\n",
    "\n",
    "# plot their eigenvalues\n",
    "fig, axs = plt.subplots(ncols=len(n_list), sharey=True, constrained_layout=True, figsize=(3 * len(n_list) + 1, 4))\n",
    "fig.suptitle(\"Singular values of $A$\")\n",
    "for A, ax, n in zip(A_list, axs, n_list):\n",
    "    svs = np.linalg.svd(A, compute_uv=False)\n",
    "    ax.plot(svs, \".\")\n",
    "    ax.set_ylabel(\"$\\sigma$\")\n",
    "    ax.set_xlabel(\"$\\sigma$ number\")\n",
    "    ax.set_title(f\"$n={n}$\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we approximate the matrix $A_n$ with different approximation methods.\n",
    "The truncated SVD gives the best approximation matrix of rank $k \\leq$ (in Frobenius norm).\n",
    "For all $k \\leq n$ compare the best approximation with Lanczos bidiagonalization method, with and without re-orthogonalization.\n",
    "\n",
    "We also measure the orthogonality error with the method from [5]:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    \\eta(U) :=  ||I -U^T U||_F\n",
    "\\end{equation}\n",
    "\n",
    "For each $k$ we also show this error for bidiagonalization with and without reorthogonalization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from linalg.helpers import get_best_approx\n",
    "\n",
    "fig, axs = plt.subplots(ncols=len(n_list), tight_layout=True, figsize=(1 + 3 * len(n_list), 4))\n",
    "axs[0].set_ylabel(\"$||A - A_k||_F$\")\n",
    "fig.suptitle(\"Error approximation methods of rank $k$ \")\n",
    "\n",
    "orth_fig, orth_axs = plt.subplots(ncols=len(n_list), sharey=True, tight_layout=True, figsize=(1 + 3 * len(n_list), 4))\n",
    "orth_fig.suptitle(\"Orthogonalization error for bidiagonalization of rank $k$\")\n",
    "orth_axs[0].set_ylabel(\"$||I - Q^{T}Q||_F$\")\n",
    "for A, n, ax, orth_ax in zip(A_list, n_list, axs, orth_axs):\n",
    "    best_approx_error = np.zeros(n)\n",
    "    bidiagonal_error = np.zeros(n)\n",
    "    bidiagonal_reorth_error = np.zeros(n)\n",
    "\n",
    "    # error: [P, Q, P_reorth, Q_reorth]\n",
    "    reorth_error = np.zeros((n, 4))\n",
    "\n",
    "    k_list = np.arange(1, n + 1)\n",
    "    for i, k in enumerate(range(1, n + 1)):\n",
    "        b = np.random.rand(n)\n",
    "        P, Q, alpha, beta = lanczos_bidiag(A, k, b)\n",
    "        B = make_bidiagonal(alpha, beta)\n",
    "        bidiagonal_error[i] = np.linalg.norm(A - P @ B @ Q.T, ord=\"fro\")\n",
    "        reorth_error[i, 0] = np.linalg.norm(np.eye(k) - P.T @ P, ord=\"fro\")\n",
    "        reorth_error[i, 1] = np.linalg.norm(np.eye(k) - Q.T @ Q, ord=\"fro\")\n",
    "\n",
    "        P, Q, alpha, beta = lanczos_bidiag_reorth(A, k, b)\n",
    "        B = make_bidiagonal(alpha, beta)\n",
    "        bidiagonal_reorth_error[i] = np.linalg.norm(A - P @ B @ Q.T, ord=\"fro\")\n",
    "        reorth_error[i, 2] = np.linalg.norm(np.eye(k) - P.T @ P, ord=\"fro\")\n",
    "        reorth_error[i, 3] = np.linalg.norm(np.eye(k) - Q.T @ Q, ord=\"fro\")\n",
    "\n",
    "        A_k = get_best_approx(A, k)\n",
    "        best_approx_error[i] = np.linalg.norm(A - A_k, ord=\"fro\")\n",
    "\n",
    "    ax.plot(k_list, best_approx_error, label=\"best approximation\")\n",
    "    ax.plot(k_list, bidiagonal_error, label=\"bidiagonalization\")\n",
    "    ax.plot(k_list, bidiagonal_reorth_error, label=\"bidiag. with reorth.\")\n",
    "    ax.set_xlabel(\"$k$\")\n",
    "    ax.set_title(f\"$n={n}$\")\n",
    "    ax.legend()\n",
    "\n",
    "    #plot orthogonality error\n",
    "    orth_ax.semilogy(k_list, reorth_error[:, 0], label=\"P\")\n",
    "    orth_ax.semilogy(k_list, reorth_error[:, 1], label=\"Q\")\n",
    "    orth_ax.semilogy(k_list, reorth_error[:, 2], label=\"P with reorth.\")\n",
    "    orth_ax.semilogy(k_list, reorth_error[:, 3], label=\"Q with reorth.\")\n",
    "    orth_ax.set_xlabel(\"$k$\")\n",
    "    orth_ax.set_title(f\"$n={n}$\")\n",
    "    orth_ax.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "From figures above we see that bidiagonalization without re-orthogonalization does not give good approximations for large $k$.\n",
    "The reason for this is that numerical instability makes the columns of $Q$ and $P$ not orthonormal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2\n",
    "### Time integration\n",
    "### Test function\n",
    "In ths exercise we consider $k$-order approximations of the solution to the matrix equation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\dot A(t) =BA(t), A(0) = A_0,\n",
    "\\end{equation}\n",
    "\n",
    "where B is a linear operator $B: \\mathbb{R}^{m \\times n} \\rightarrow \\mathbb{R}^{m \\times n}$.\n",
    "This equation will have solution $A(t) = \\exp({tB})A_0$.\n",
    "\n",
    "In our test problem we let B be the discrete laplacian operator $L_h$.\n",
    "Thus, $A(t)$ will be the time evolution of the space discretized heat equation in two space dimensions;\n",
    "with $m,n$ nodes in the $x, y$ directions respectively.\n",
    "Now computing the matrix exponential will be an expensive operation with cost $ O((nm)^{3}$). Therefore, we transform the original ODE to a ODE with only matrix products.\n",
    "\n",
    "For any time $t$ a discretized heat matrix can be considered as the sum of outer products\n",
    "\n",
    "\\begin{equation}\n",
    "A(t) = \\sum_{i = 1}^{\\min(m,n)} u_i \\otimes v_i,\n",
    "\\end{equation}\n",
    "\n",
    "where $u_i$ and $v_i$ are a vectors variying in the $x$ and $y$ directions respectively.\n",
    "Now since the discrete laplacian is the sum of the double derivative in each directon it can be decomposed as:\n",
    "\n",
    "\\begin{equation}\n",
    "L_h(A(t))= \\sum_{i = 1}^{\\min(m,n)} D_{xx} u_i \\otimes v_i +\\sum_{i  = 1}^{\\min(m,n)} u_i \\otimes D_{yy} v_i = D_{xx}A(t) + A(t)D_{yy}^T.\n",
    "\\end{equation}\n",
    "\n",
    "Here $D_{xx}$ and $D_{yy}$ are the discrete laplacians in one dimension.\n",
    "Thus, the solutoin to the matrix ODE with $B = L_h$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "A(t)= \\exp(tD_{xx})A_0 \\exp(tD_{yy}^T).\n",
    "\\end{equation}\n",
    "\n",
    "We now only have to compute these two exponentials with cost $O(m^3+ n^3)$.\n",
    "\n",
    "### Efficient computation of the caylay map\n",
    "\n",
    "### Testing the caylay map with the heat eqatoin\n",
    "\n",
    "### Approximatons of the heat eqation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from linalg.integrate import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To make sure that the resulting matrices are in fact orthogonal we take steps in using the caylay-map.\n",
    "Furthermore, all inputs in they caylay map are given on the form $B = [F, -U] [U, F]^T$.\n",
    "Where $U^TU = I$ and $F^T U=0$\n",
    "This input form we can exploit to compute the caylay map more efficiently:\n",
    "..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from linalg.cayley_map import cayley_map_simple, cayley_map_plus, cayley_map_efficient"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now compare the performance of the different ways to compute the caylay map.\n",
    "The cases are constructed by first generating matrices $A$ and $B$.\n",
    "$A$ and $B$ are random matrices with elements randomly drawn from the uniform distribution on [0,1].\n",
    "Then we compute the QR factorization of $A$ and set $Q$ as the $U$ matrix.\n",
    "Then $F$ is computed as $F:=(I - U U^T)G $"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from test.test_caylay import get_FUCDB\n",
    "\n",
    "m_list = 2 ** np.array([4, 5, 6, 7, 8, 9, 10, 11])\n",
    "sample_FUCDB = [get_FUCDB(m, k=int(np.sqrt(m))) for m in m_list]\n",
    "sample_FU = [(F, U) for F, U, C, D, B in sample_FUCDB]\n",
    "sample_CD = [(C, D) for F, U, C, D, B in sample_FUCDB]\n",
    "sample_B = [(B,) for F, U, C, D, B in sample_FUCDB]\n",
    "\n",
    "time_simple = get_function_timings(cayley_map_simple, sample_B, number=10)\n",
    "time_efficient = get_function_timings(cayley_map_efficient, sample_CD, number=10)\n",
    "time_plus = get_function_timings(cayley_map_plus, sample_FU, number=10)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"Performance of different methods of caylay map computation\")\n",
    "ax.loglog(m_list, time_simple, label=\"simple\", base=2)\n",
    "ax.loglog(m_list, time_efficient, label=\"efficient\", base=2)\n",
    "ax.loglog(m_list, time_plus, label=\"efficient modified\", base=2)\n",
    "\n",
    "ax.set_ylabel(\"Time [ms]\")\n",
    "ax.set_xlabel(\"$m$\")\n",
    "ax.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that method caylay map implementation inverting the smallest matrix is the most efficient implementaiton for our case."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from test.case_matrix_ode import generate_heat_equation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now test our ..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m = 20\n",
    "t_f = 1\n",
    "k_list = [5, 15]\n",
    "fig, axs = plt.subplots(ncols=len(k_list), nrows=2, sharex=True, constrained_layout=True,\n",
    "                        figsize=(1 + 3 * len(k_list), 4 + 3))\n",
    "axs[0, 0].set_ylabel(\"Frobenious norm\")\n",
    "axs[1, 0].set_ylabel(\"Frobenious norm\")\n",
    "fig.suptitle(\"Error for different low rank approximations\")\n",
    "from linalg.integrate import get_y_dot\n",
    "\n",
    "for i, k in enumerate(k_list):\n",
    "    print(f\"Running k={k}\")\n",
    "    # generate case and start conditions\n",
    "    A_0, A, A_dot = generate_heat_equation(n=m, m=m, k=k)\n",
    "    Y_0 = truncated_svd(A_0, k)\n",
    "\n",
    "    # integrate\n",
    "    Y, T = matrix_ode_simple(0, t_f, Y_0=Y_0, X=A_dot, TOL=1e-3, verbose=True)\n",
    "    t_ind = get_equidistant_indexes(T, 0, t_f)\n",
    "    T = [T[i] for i in t_ind]\n",
    "    Y = [Y[i] for i in t_ind]\n",
    "\n",
    "    XA_diff = [np.linalg.norm(get_best_approx(A(t), k) - A(t), ord=\"fro\") for t in T]\n",
    "    YA_diff = [np.linalg.norm(multiply_factorized(*y) - A(t), ord=\"fro\") for t, y in zip(T, Y)]\n",
    "    YA_dot_diff = [np.linalg.norm(get_y_dot(A_dot=A_dot, Y=y, t=t) - A_dot(t), ord=\"fro\") for t, y in zip(T, Y)]\n",
    "    YX_diff = [np.linalg.norm(multiply_factorized(*y) - get_best_approx(A(t), k), ord=\"fro\") for t, y in zip(T, Y)]\n",
    "\n",
    "    ax_u = axs[0, i]\n",
    "    ax_u.set_title(f\"$k={k}$\")\n",
    "    ax_u.plot(T, YX_diff, label=\"||Y - X||\")\n",
    "    ax_u.plot(T, YA_diff, label=\"||Y - A||\")\n",
    "    ax_u.plot(T, YA_dot_diff, label=\"$||\\dot{Y} - \\dot{A}||$\")\n",
    "    ax_u.legend()\n",
    "\n",
    "    ax_l = axs[1, i]\n",
    "    ax_l.plot(T, XA_diff, label=\"||X - A||\")\n",
    "    ax_l.set_xlabel(\"$t$\")\n",
    "    ax_l.legend()\n",
    "    clear_output()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that the best approximation of rank $k$ will perfectly approximate A wich also has rank $k$.\n",
    "$Y$ does not approximate as well as the best approximate."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 3\n",
    "We now consider the first example in section 3 of [4]. Here the matrix to be approximated is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "ÅA(t) = Q_1(t)\\bigg(A_1 + e^{t}A_2\\bigg)Q_2^T(t).\n",
    "\\end{equation}\n",
    "\n",
    "Here $Q_i$ are orthonormal matrices given by $\\dot Q_i= T_i Q_i, \\ i=1,2$,\n",
    "where $T_i$ are skew symmetric matrices with constant diagonals.\n",
    "$A_1$ and $A_2$ are randomly generated matrices in $[0,5]^{100 \\times 100}$ with 10 singular values ≈ 1.\n",
    "$\\epsilon$ is a prameter regulating how much noise is added to the $A_1$ and $A_2$ matrices.\n",
    "See [4] for further details.\n",
    "\n",
    "The test problmen is implemented in the cell below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from test.case_matrix_ode import generate_first_example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the test function defined above, we test the following approximation methods:\n",
    "* Truncated SVD (best approximation of order $k$)\n",
    "* Lanczos bidiagonalization method\n",
    "* Dynamic low-rank approximation\n",
    "\n",
    "Each approximation method of order $k \\in \\{10,20\\}$ ws tested with a degree of added noice $\\epsilon \\in \\{1e-1, 1e-2, 1e-3, 1e-4, 1e-5\\}$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t_f = 1\n",
    "eps_list = 10. ** np.array([-1, -2, -3, -4, -5])\n",
    "k_list = [10, 20]\n",
    "m = 100\n",
    "fig, axs = plt.subplots(nrows=len(eps_list), ncols=len(k_list), sharex=True, squeeze=False, constrained_layout=True,\n",
    "                        figsize=(1 + 3 * len(k_list), 1 + 3 * len(eps_list)))\n",
    "\n",
    "fig.suptitle(\"Error for different low rank approximations\")\n",
    "for i, eps in enumerate(eps_list):\n",
    "    axs[i, 0].set_ylabel(\"Frobenious norm\")\n",
    "    for j, k in enumerate(k_list):\n",
    "        axs[-1, j].set_xlabel(\"$t$\")\n",
    "        # generate case and start conditions\n",
    "        print(f\"k: {k}, epsilon: {eps}\")\n",
    "        A_0, A, A_dot = generate_first_example(eps=eps)\n",
    "        Y_0 = truncated_svd(A_0, k)\n",
    "\n",
    "        # integrate\n",
    "        Y, T = matrix_ode_simple(0, t_f, Y_0=Y_0, X=A_dot, TOL=1e-3, verbose=True)\n",
    "\n",
    "        t_ind = get_equidistant_indexes(T, 0, t_f, n=m)\n",
    "        T = [T[i] for i in t_ind]\n",
    "        Y = [Y[i] for i in t_ind]\n",
    "\n",
    "        # I know this is not the most efficient way but it is easy to read\n",
    "        b = np.random.rand(m)\n",
    "        XA_diff = [np.linalg.norm(get_best_approx(A(t), k) - A(t), ord=\"fro\") for t in T]\n",
    "        YA_diff = [np.linalg.norm(multiply_factorized(*y) - A(t), ord=\"fro\") for t, y in zip(T, Y)]\n",
    "        YA_dot_diff = [np.linalg.norm(get_y_dot(A_dot=A_dot, Y=y, t=t) - A_dot(t), ord=\"fro\") for t, y in zip(T, Y)]\n",
    "        WA_diff = [np.linalg.norm(get_bidiagonal_approx(A(t), k=k, b=b) - A(t), ord=\"fro\") for t in T]\n",
    "        YX_diff = [np.linalg.norm(multiply_factorized(*y) - get_best_approx(A(t), k), ord=\"fro\") for t, y in zip(T, Y)]\n",
    "        ax = axs[i, j]\n",
    "        ax.set_title(f\"$k=${k}, $\\epsilon =$ {eps}\")\n",
    "        ax.plot(T, XA_diff, label=\"$||X - A||$\")\n",
    "        ax.plot(T, YA_diff, label=\"$||Y - A||$\")\n",
    "        ax.plot(T, YA_dot_diff, label=\"$||\\dot{Y} - \\dot{A}||$\")\n",
    "        ax.plot(T, WA_diff, label=\"$||W - A||$\")\n",
    "        ax.plot(T, YX_diff, label=\"$||Y - X||$\")\n",
    "        ax.legend()\n",
    "        clear_output()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Effektive rank 10. More like a ank 10 matrix\n",
    "Compared to the bidiagnoalizaon. low approsi\n",
    "\n",
    "For larger $k$ the lanchos bidiagonalizaton is better. Needs more dimentions to capture the same singular values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 4\n",
    "We now implement the second example in section 3 of [4]. Here $A(t)$ is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "A(t) = Q_1(t)\\bigg(A_1 + \\cos(t)A_2\\bigg)Q_2^T(t),\n",
    "\\end{equation}\n",
    "\n",
    "where $Q_i$ and $A_i, \\ i={1,2}$ are defined as in the previous example.\n",
    "Notice, however, that $e^t$ has been replaced by $\\cos(t)$\n",
    "\n",
    "The exaple is immplemented in the code block below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from test.case_matrix_ode import generate_second_example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now test the the dynamical low-rank approximation $Y(t)$ and truncated SVD $X(t)$ of order $k=\\{5,20\\}$ on the test problem with $\\epsilon = 0.1$.\n",
    "We also find the singular values of each approximation method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t_f = 10\n",
    "eps_list = [1e-1]\n",
    "k_list = [5, 20]\n",
    "m = 100\n",
    "fig_sigma, axs_sigma = plt.subplots(nrows=len(eps_list), ncols=len(k_list), sharex=True, sharey=True, squeeze=False,\n",
    "                                    constrained_layout=True,\n",
    "                                    figsize=(1 + 3 * len(k_list), 1 + 3 * len(eps_list)))\n",
    "fig_sigma.suptitle(\"Singular values over time\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(eps_list), ncols=len(k_list), sharex=True, squeeze=False, constrained_layout=True,\n",
    "                        figsize=(1 + 3 * len(k_list), 1 + 3 * len(eps_list)))\n",
    "fig.suptitle(\"Error for different low rank approximations\")\n",
    "for i, eps in enumerate(eps_list):\n",
    "    axs[i, 0].set_ylabel(\"Frobenious norm\")\n",
    "    for j, k in enumerate(k_list):\n",
    "        axs[-1, j].set_xlabel(\"$t$\")\n",
    "        axs_sigma[-1, j].set_xlabel(\"$t$\")\n",
    "        # generate case and start conditions\n",
    "        print(f\"k: {k}, epsilon: {eps}\")\n",
    "        A_0, A, A_dot = generate_second_example(eps=eps)\n",
    "        Y_0 = truncated_svd(A_0, k)\n",
    "        # integrate\n",
    "        Y, T = matrix_ode_simple(0, t_f, Y_0=Y_0, X=A_dot, TOL=1e-3, verbose=True)\n",
    "\n",
    "        # store a subset instead\n",
    "        t_ind = get_equidistant_indexes(T, 0, t_f, n=2 * m)\n",
    "        T = [T[i] for i in t_ind]\n",
    "        Y = [Y[i] for i in t_ind]\n",
    "\n",
    "        # I know this is not the most efficient way but it is easy to read\n",
    "        b = np.random.rand(m)\n",
    "        XA_diff = [np.linalg.norm(get_best_approx(A(t), k) - A(t), ord=\"fro\") for t in T]\n",
    "        YA_diff = [np.linalg.norm(multiply_factorized(*y) - A(t), ord=\"fro\") for t, y in zip(T, Y)]\n",
    "        YA_dot_diff = [np.linalg.norm(get_y_dot(A_dot=A_dot, Y=y, t=t) - A_dot(t), ord=\"fro\") for t, y in zip(T, Y)]\n",
    "        YX_diff = [np.linalg.norm(multiply_factorized(*y) - get_best_approx(A(t), k), ord=\"fro\") for t, y in zip(T, Y)]\n",
    "        A_norm = np.array([np.linalg.norm(A(t), ord=\"fro\") for t in T])\n",
    "        ax = axs[i, j]\n",
    "        ax.set_title(f\"$k=${k}, $\\epsilon =$ {eps}\")\n",
    "        ax.plot(T, XA_diff, label=\"||X - A||\")\n",
    "        ax.plot(T, YA_diff, label=\"||Y - A||\")\n",
    "        ax.plot(T, YA_dot_diff, label=\"$||\\dot{Y} - \\dot{A}||$\")\n",
    "        ax.plot(T, YX_diff, label=\"||Y - X||\")\n",
    "        ax.legend()\n",
    "\n",
    "        sing_values = np.linalg.svd([A(t) for t in T], compute_uv=False)\n",
    "        sing_values_y = np.linalg.svd([S for U, S, V in Y], compute_uv=False)\n",
    "        ax_sigma = axs_sigma[i, j]\n",
    "        ax_sigma.set_ylabel(\"$\\sigma_i$\")\n",
    "        ax_sigma.set_title(f\"$k=${k}, $\\epsilon =$ {eps}\")\n",
    "        l0, *_ = ax_sigma.plot(T, sing_values, \"k-\", lw=0.5, label=\"A(t)\")\n",
    "        l1, *_ = ax_sigma.plot(T[::4], sing_values_y[::4, :k], \"r.\", alpha=0.5, label=\"Y(t)\")\n",
    "        ax_sigma.legend(handles=[l0, l1])\n",
    "\n",
    "        clear_output()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This example illustrates how the dynamical low rank approximation $Y(t)$ as no error bounds in general.\n",
    "The authors of [4] give error bound with under regularity conditions for the best-approximation $X(t)$.\n",
    "In this case however, the rank $k=5$ best-approximation $X(t)$ is discontinuous, and thus we have no error bounds.\n",
    "\n",
    "More specifically the low rank approximation fails because the singular values of $A(t)$ cross multiple times.\n",
    "To see how this affects the low rank approximation we consider an initial ordering of the singular values of $A(0)$ as $\\sigma_i, 1 \\leq i \\leq 100$.\n",
    "Now since $A$ has the form\n",
    "\n",
    "\\begin{equation}\n",
    "A(t) = Q_1(t)\\bigg(A_1 + \\cos(t)A_2\\bigg)Q_2^T(t),\n",
    "\\end{equation}\n",
    "\n",
    "we can write $A(t)$ as sum of smooth functions\n",
    "\n",
    "\\begin{equation}\n",
    "A(t) = \\sum_{i=1}^{100} u_i(t)v_i(t)^T \\sigma_i (t).\n",
    "\\end{equation}\n",
    "\n",
    "Now let $t_k$ be the first time when $\\sigma_{j}(t) \\leq \\sigma_{i}(t), j \\leq k < i $.\n",
    "Then for $0 \\leq t<t_k$:\n",
    "\n",
    "\\begin{equation}\n",
    "X(t) = \\sum_{i=1}^{k} u_i(t)v_i(t)^T \\sigma_i (t),\n",
    "\\end{equation}\n",
    "\n",
    "but at $t_k$ $X$ will change functions with index $i$ for index $j$ and have a discontinuity. We know it has to be a discontinuity because all $u_i$'s and $v_i$'s are smooth functions pointwise orthogonal to each other.\n",
    "On the other hand, the dynamical low rank approximation is smooth and will thus approximate the $k$ singular values that where largest,\n",
    "even though they are not largest anymore.\n",
    "\n",
    "This crossing singular values can be seen in figure above.\n",
    "Here we see that when singular values not approximated by $Y(t)$ becomes large.\n",
    "Then the approximated singular stops to follow the curve of singular values of $A(t)$.\n",
    "Also in the figure below the errors of different approximations are shown  se that when singular values cross.\n",
    "Then the difference between the best approximation $X$ and the dynamical low rank approximation  $Y$ grows larger.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}